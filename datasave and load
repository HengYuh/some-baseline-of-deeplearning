模型的保存与加载
  import torch
  torch.save(model.state_dict(),"model/model_name.pt")
  torch.save(optimizer.state_dict(),"results/model_name.pt")       #save model and optimiezer ，return a dict
  
  model_name.load_state_dict(torch.load("model/model_name.pt"))
  optimizer.load_state_dict(torch.load("results/model_name.pt"))   #load model and optimiezr，return a dict







分类问题的准确率提取
  output=model(x)
  pred=output.max(dim=-1)[-1]  #返回值有俩，一个为最大值，一个为最大值的下标，
                                 # 后一个-1代表只接收下标，即这个输入最有可能是什么下标
  acc=pred.eq(target).float().mean() #将布尔类型转换为浮点数，相等则转化为1.0，否则0.0，然后求均值即为准确度
                     示例
                    import torch
                    
                    # 假设模型输出（每个样本对应三个类别的分数）
                    output = torch.tensor([[2.5, 1.0, 0.1],
                                           [0.2, 1.8, 2.2],
                                           [1.5, 1.6, 1.4]])
                    
                    # 输出的张量的形状
                    print(output.shape)
                    在这个示例中，output 是一个形状为 (3, 3) 的张量，表示有3个样本和3个类别。每行表示一个样本，每列表示一个类别，张量中的值是模型对每个类别的分数。
                    
                    然后，我们可以使用 output.max(dim=-1) 操作来找到每个样本中具有最高分数的类别的索引。这里 dim=-1 表示在最后一个维度上执行操作，即在每个样本的维度上执行操作。
                    
                    python
                    Copy code
                    # 找到每个样本中具有最高分数的类别的索引
                    pred = output.max(dim=-1)[-1]
                    
                    # 预测结果的张量
                    print(pred)
                    这将返回一个包含每个样本的预测类别索引的张量，结果如下：
                    
                    scss
                    Copy code
                    tensor([0, 2, 1])
                    在这个示例中，模型对第一个样本预测为类别 0，对第二个样本预测为类别 2，对第三个样本预测为类别 1。
                    
                    最后，我们可以计算准确率，即模型正确预测的样本数占总样本数的比例：
                    
                    python
                    Copy code
                    # 真实标签（假设真实标签）
                    target = torch.tensor([0, 2, 1])
                    
                    # 计算准确率
                    acc = pred.eq(target).float().mean()
                    print(acc.item())  # 打印准确率值
                    这将计算出准确率，并打印出来。在这个示例中，如果假设真实标签为 [0, 2, 1]，那么模型的准确率为 100%（1.0）。









LSTM
  batch_size=10   #句子的数量
  seq_len=20       #每个句子的长度，需要统一，但是注意不能太大，不然会导致acc一直不变化（未明白为什么）
  embedding_dim=30      #每个词语使用多长的向量表示，一般为512这些数，大一些会比较好
  word_vocab=100      #一般要到万的级别，这个100太小了
  hidden_size=18      #隐藏神经元的个数
  num_layer=2        #层数
  
  input=torch.randint(low=0,high=100,size=(batch_size,seq_len))
  embedding=torch.nn.Embedding(word_vocab,embedding_dim)
  lstm=torch.nn.LSTM(embedding_dim,hidden_size,num_layer,bidirectional=True)
  
  embed=embedding(input)  #[10,20,30]
  #转化数据为batch_size=False
  embed=embed.permute(1,0,2) #[20,10,30]
  h_0=torch.rand(num_layer*2,batch_size,hidden_size)
  c_0=torch.rand(num_layer*2,batch_size,hidden_size)
  output,(h_1.c_1)=lstm(embed,(h_0,c_0))

#结果的获取,获取最后一个细胞的状态即可
  若batch_first=False
  output[-1] #or output[-1,:,:]
  or
  output[:,-1,:]

